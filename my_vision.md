The next step is to create a plan for how the final system should look like. My goal is actually to have a two agent system. Both agents should be running in parallel working on their task is asynchronously.

Architecture wise I think we should divide the src folder into creator and validator agent directories. These two directories can share code like the tools (perhaps we keep them in the src folder since both might want to have websearch, and they definately need to share the citation tool, etc...). I guess we can setup the docker files in a way that they just copy what each individual agent needs. Because I'd like to move away from one large application, towards having three seperate deployments/containers (not including databases).

- Main Application: This should be the main "orchestrator" that will be used to start/stop and coordinate both agents. We can view this as our main function (for development we can just keep that as the main and only build both agents as containers to be run in parallel, this way we can run the main as a python script and have it send requests to both agents and so on).

- Creator Agent: The creator agent will be one of our two agents for this project. His job will be to extract requirements from the sources he is provided with. I'm still unsure what autonomy level we should give the agents. As of now I think we have two options: 1. The creator agent coontainer recives the entire document and keeps work on it untill all requirements are extracted (e.g. he will contain all the logic for dividing large documents into smaller chunks, looking for rquirement candidates and then iterating over each candidate to create a propper requirement from a possible candidate). 2. Or should the main application handle the "candidates extraction process" and simply pass the requirement candidates to the creator agent, which will then go ahead and generate a propper requirement from the candidate and store it in the database. In any way, the creator agent should handle the task of creating well documented and thought through requirements based on the source material he has been provided (in combination with vector and web search if additional information is needed ofcourse). He will recive access to the neo4j database (I think both agents should have all the tools/information avaialable for now, we can later restrict them if we found that one of them dialates to much from his intended role, e.g. the creator agent doing the job of the validator agent as well for example). He will also recive the source material that contains the requirements. This could be a 50 page legal text (GDPR for example) or just a single sentence, concept or idea ("What would happen if our company would implement a digital twin?" for example). He would then start to create "requirements" for the topic. This might include different aproaches. On simple questions such as "What if we implemented a digital twin in our company?" he might start by expanding the topic first to create a document containing information about this topic (e.g. simple questions might require to create a document using websearch and many iterations). While if he would recive a large document such as the GDPR for example, he might start to work on breaking down this document into smaller chunks (e.g. split chapters, look into every chapter seperately, maybe dissassemble the text and group different sections into topic that might be related and should be processed together or whatever).

- Validator Agent: The second agent of our two agent system will be the counterpart to the first agent. While the creator agent should work on "creating requirements" the second agent will focus on validating these requirements. So after the creator agent has saved a requirement to the database (I suggest we use a postgres database with a requirements table as a shared cache for requirements, e.g. creator agent creates and saves requirements to it, validator agent picks them up validates them and marks them as done once they have been processed). So the validator agent is the "original" agent we first build (while the creator agent is the new one we recently added). The validaor agent should take a requirement and process it using the neo4j database, websearch and so on. His goal should be to check whether the given requirement can be applied to the company and if so, whether it is fullfilled or not. If it is not fullfilled he should create a requirement node in the neo4j database and connect it with applies to or is connected to business objects who are directly related to the requirement. Should the requirement be fullfilled he should do the same but with a fullfills relationship instead. In many cases the agent will probably end up having to do both (e.g. create a requirement node and connect it to business objects that already fullfill the requirement via a fullfills relationship and connect it to those business objects who do not fullfill the requirement with does not fullfill requirement relationships). Perhaps we need to revisit the neo4j schema and how to propperly do the requirement to business object relationships and so on.

As you see there is a lot to talk about, and many different aspects to explore. Perhaps we should also run a few reseach agents on how to implement autonomous agents that can run idefinately (e.g. context window management, how to implement to do list management for the agent, make sure he can split a document into chunks, then go process one chunk, store the result somewhere and go ahead to process the next chunk without his context window being bloated by the converstion of processing the first chunk).

The overall workflow should be as follows:
1. The main application is started and the process is initiated by the user who will provide the nessesary input (document + prompt, e.g. "Hey check out the GDPR. Our company is from the US and now wants to start a branch in europe, I have attached the GDPR documents for reference."). This can either be done via a UI or for simplicity I'd like to ignore UI topics for now and just focus on getting the backend server right (e.g. let's just run the primary workflow with python start_orchestrator --prompt "..." --document-path <path_to_document>)

2. The main application now creates a new "task/job" entry in the database (so we can track resource ussage on a job). The job table entry should have an ID and store the input prompt, document and metadata such as start date and so on (that ID can then link to other tables that store resource ussage on the two agents, e.g. requests, tool calls tokens used and so on, we could even dump all the llm requests that are returned from the open ai api schema into a seperate mongodb, my home lab has 30tb storage so we don't have to worry about that right now, better to store everything for later evaluation, in the end we're still in development phase).

3. After the initialization is done, the main application should now send a request to the creator agent (again still unsure if we send the whole document or have the main application create candidates). Right now I'm leaning more towards the creator agent reciving the whole document with prompt and then starting to break it down, do all the preprocessing (create requirement candidates, process each one individually, use sources and websearch to create propper requirements, store them in the database, move on to the next requirement candidate). Because the goal I have is that the requirements sholdn't arrive all at the same time, but the agent should first do the preprocessing (e.g. create a list of rough candidates), then go over that list and really take time to investigate and propperly create a well documented requirement based on literature and so on. This means that he might spend 30 minutes or an hour up front to break this document down into smaller chunks that could potentially lead to an requirement. He might also explore the web, read up on details, really work it's way through to create a comprenensive knowledge base.

4. After that (e.g. when the agent decided he is done creating the list of candidates), he should now start to process the requirement candidates list one at a time. The list could be a document which he created during step 3 and stored in his project folder, or it could be a seperate db table that he can read/write to. Because now, we are going to take those requirement candidates and save them in a vector database or store them in the neo4j database with the creator agents user so the other AI won't get confused by accidently querying them. Because now the creator agent should pick on of the requirement candidates and try to create a requirement based on this candidate. And here again, he should really take his time to do a propper job. Iterate as often as needed, use all sources (e.g. search the vector or graph db for similar candidates so he can be sure that we don't create the same requirement twice because we made similar candidates, but again her, the agent should take his time, perhaps the desicion if three candidates should be represented as one requirement or two or three might take another 20 minutes, why not?, I have a oss 120b model running on my home lab, we have so many GPUs at the university and the LLMs are never used to full capacity so let's really build an autonomous system that might take a day or two or perhaps a week to get one job done!). Having said that the agent should now collect as much knowledge as needed to create a propperly formulated requirement that isn't covered by other requirement candidates (e.g. eliminate duplication), is propperly described (so the other agent has all the information he needs about the requirement to verify and apply it to the companies specific situation) and that is based on sources (e.g. the desicions should be tracable, the AI should use the citation tool to cite sources and propperly argue it's position/desicions).

5. After the creator agent is has finished fletching out a requirement, he should save it to the shared postgres db table and move on to process the next requirement candidate. Once the requirement has been added to the database the validator agent will now pick it up and start to validate it. He might do so by quickly checking if everything is correct (perhaps we can skip that part since the citations are automatically validated anyways), and if the requirement can be applied to the company. He might do this by checking the database and exploring neo4j to see if the requirement can be applied or if the business objects (or concepts, or whatever the requirement would address) don't exist in the neo4j graph. In this case he might just mark the requirement as not relevant and would add a comment why he made this desicion (again, this should be propperly argued based on sources). However if the agent decides that the requirement is relevant and should be incerted into the neo4j graph, then he should prepare the insertion of the requirement. He might do so by really diving deep into the graph and examining to which business objects or other requirements the requirement node should be connected to and how it should be connected. This could be done step by step iteating through nodes to see if they would need a related, fullfilled or not fullfilled realtionship and why. The agent again might take a long time to process this one requirement, but that is fine since we are not building a system for speed, but one that can make well reasoned, rational an thought through desicions who are based on facts and so on.

6. Once the agent has finished adding all the nodes and relationships (a requirement perhaps could lead to more than just one nodem, depending on the schema we design). He should then go ahead and validate that the graph is still compliant to the metamodel schema using the db validation tools. This could involve checking syntax or looking for possible redundancies or revisiting the newly incerted nodes and relationsships and checking if they are correct (duplication, redundency, perhaps he documented something along the way or wrote a note that has been forgotten and might still need further investigation). Should all check have passed and he has "cleaned up his workspace" cheked all notes and so on. The changes would be commited to the neo4j graph (I don't know if they have a function like that or if we would want to build that I don't know, but perhaps we might need rollback functionality in case the agent notices that he has made a mistake). One done he should move on to process the next requirement from the shared requirement table/cache.

7. This process should run untill the creator agent has finished processing the requirements (e.g. he has done his job) and the validator agent has processed all of the requirements created by the creator agent (e.g. he has done his job). Should that be the case then I guess the job is done and the main application can "mark it as complete". I'm not sure about these aspects. Perhaps we can store a bool for the creator agent in the job table that indecates wheter he is fully done or not (same could be done for the validator agent, perhaps we might track their status and if both are on status ready, then we would know there are not new requirements who are getting created and no more existing requirements who are waiting to be verified). In this case we can mark the process as complete and that's it. Not that due to the asynchronous nature of the system there really isn't an end for all of this. Since once we know the system works (e.g. we have eliminated all the small bugs and proven the concept), this concept could be scaled up to include multiple creator/validator agents at the same time. We could acually scale it up to run multiple jobs in parallel and have the system pick and choose agents from a pool to work on different kinds of jobs. Perhaps we could build a scheduler into the main application that could adjust the number of creator and validator agents based on demand (at times the application might want to run more creator agents to fill up a buffer of requirements before going back to 50/50 distribution or perhpas switch over to only validator agents should the creator agents have ended with a large pile of requirements still to be validated). But this is something we might look into later on. For now we just focus on setting up everything to work with 1 agent each.

Another point about the scope. I mean, I have a bough really expensive hardware to run local AI on my home lab. But this just resulted in the hardware beeing used for perhaps 30 minutes a day (in total over a 24 hour time period). So, I'm really looking to build tools that can run for days or weeks. I got a request scheduler that can give the requests from this tool lower priority so they are just used to fill the gaps and when I or my family need my AI the requests will have to wait. That's not a big problem and it helps to keep my expensive infrastructure buys. In the end I don't wanna spend big money on hardware that is idle most of the time. This tool would be a great candidate to fill the gaps over a long period of time, so keep that in mind when designing the agents. If there is one thing I don't want to have with tasks as large as this, and that's the realization that some aspects have slipped through because the AI didn't go the extra mile.

Also, about the requirements. I always refer to them as requirements, but these can be more than just that (e.g. they don't need to be specific or numeric, they could be a high level concept that needs to be represented with multiple nodes in the neo4j db). Or maybe we should do it so that the first agent only produces requirements on a smaller scale (e.g. he is instructed to breakt down larger problems into smaller requirements: GDPR needs data privacy -> we must protect user data -> we need strong encryption against outside enemies, we don't store master data that can authenticate a person directly via their name or social id, our data analysts only receive pseudonymized data -> our sql views need to support personas and our db administration team needs to be trained appropriately to know how to work with sensitive data). The validator agent would then take these requirements and might check which department the db admins are part of and see what needs to be added where (e.g. how do we make sure they get trained to work with sensitive data, do we have a compliance team? -> add a requirement that compliance team needs to ensure all departments recive regualar trainings, -> add a note "must ensure regular trainings about various topics such as occupational safety, fire safety, data protection or antitrust laws", etc.)

Please create a plan for such an infrastructure (you can do it as concept.md). The plan should describe what we want to achive (e.g. a short introduction and some context about the goal) and a high level picture on what we want to build (e.g. the two agents working in parallel to create requirements and verify/add them to the db).
