1+2. These are good question, go check the web to find a solution for that.
3. Well, my idea was more that we have a task table and a job table. Both should behave like a paper stack. User creates a task -> Entry on the job table is created. When idel the agents can either run a "event loop" that checks the db table for new entries (e.g. creation agent checks for new jobs, should the db return a job where complete == false then he should load that job and get to work, same with validation agent, he checks the requirements table for requirements where complete == false and picks one if the table has one). This would be an option. The other option would be to send a notification once a job was created or a requirement has been added, but I think we can just let the agents (since they are run in seperate pods) query the db for open tasks (e.g. "a paper stack sytem", agents sit next to each other and once one is finished he puts the sheet on a paper stack to his right from which the other agent can pick a new task once he finished his, if the desk to his left is empty he knows there is nothing to do, so he waits and checks every now and then if he's got a new assignment).
4. My home lab runs a k3s cluster. Every agent should sit in a pod that can send requests to the other pods (we can build a basic communications using fast API). The llm can be accessed through another deployment on my cluster (it's just a llama.cpp open ai api compatible deployment). So, just setup everything in containers and create a docker compose. I will take that and handle all the dev ops deployment on kubernetes my self (that's not part of the project). You can create what ever API you want. Make sure to also include endpoints for health checks that can be used by kubernetes and so on.
5. Yes, the existing code can and should be reused. But for now we're just planning the result and leave this aspect to be covered in the implementation roadmap.
6. This was a question I have asked my self as I brainstormed the application concept. I'm not sure if we really need this feature, perhaps we can add a requirement tag that is stored in postgres reccord of the requirement. This way we could easily trace what has been added by what requirement. It was less driven by the idea to rollback, but more by the idea that the agent might want to revisit all the changes he has made during the validation of a requirement to recapture his work and see if everyting is still correct.
7. The CitationEngine is a self sustained module, perhaps we should set it up to use the same postgres db as the agents. The configuration is done via environment variables (how this is implemented should be left to the implementation roadmap or the dev who picks the ticket). It should just be noted that the CitationEngine should be used as a python package so development on that one can be done seperately (since the CitationEngine is used in multiple of my projects).
8. We will not implement this here. This is something I've implemented in my model deployment and isn't part of this project.
9. Yes, indeed! Please check the web on this one and specify a few potential aproaches and so on.
10. Perhaps we should revisit the metamodell.cql and create a proper schema for the neo4j (based on the metamodell ofcourse).
