# =============================================================================
# Graph-RAG Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values

# =============================================================================
# Required: LLM Configuration
# =============================================================================

# OpenAI API Key (or compatible API)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Custom OpenAI-compatible endpoint (vLLM, Ollama, llama.cpp)
# Leave unset to use OpenAI's API directly
# LLM_BASE_URL=http://your-server:8000/v1

# =============================================================================
# Required: PostgreSQL Database
# =============================================================================

# Connection URL (preferred)
DATABASE_URL=postgresql://graphrag:graphrag_password@localhost:5432/graphrag

# Or individual parameters (used if DATABASE_URL not set)
# POSTGRES_HOST=localhost
# POSTGRES_USER=graphrag
# POSTGRES_PASSWORD=graphrag_password
# POSTGRES_DB=graphrag

# =============================================================================
# Optional: Additional Services
# =============================================================================

# Neo4j (Knowledge Graph) - only if connections.neo4j: true in config
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=neo4j_password

# MongoDB (LLM request archiving) - enables audit trail
# MONGODB_URL=mongodb://localhost:27017/graphrag_logs

# Tavily (Web Search) - required for web_search tool
# TAVILY_API_KEY=your_tavily_api_key_here

# Anthropic (Claude models) - required when using claude-* models
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google (Gemini models) - required when using gemini-* models
# GOOGLE_API_KEY=your_google_api_key_here

# Groq (Fast inference) - required when provider: groq
# GROQ_API_KEY=your_groq_api_key_here

# =============================================================================
# Optional: Vision Model (for describing visual content)
# =============================================================================
# Used when the primary model is text-only (llm.multimodal: false in config).
# A separate multimodal model generates text descriptions of images and
# document pages (charts, diagrams, figures).
#
# If not configured, falls back to OPENAI_API_KEY and default OpenAI endpoint.

# Vision model API key (defaults to OPENAI_API_KEY)
# VISION_API_KEY=your_openai_api_key_here

# Vision model base URL (defaults to OPENAI_BASE_URL or OpenAI's API)
# VISION_BASE_URL=https://api.openai.com/v1

# Vision model to use (default: gpt-4o-mini - fast and cost-effective)
# Options: gpt-4o-mini, gpt-4o, gpt-4-turbo, or any multimodal model
# VISION_MODEL=gpt-4o-mini

# Request timeout in seconds (default: 120)
# VISION_TIMEOUT=120

# =============================================================================
# Optional: Audio Transcription (Whisper)
# =============================================================================
# Used for transcribing audio files. Supports OpenAI's Whisper API or
# local whisper model as fallback.
#
# Note: Audio transcription is not yet implemented in read_file tool.
# These settings are reserved for future use.

# Enable audio transcription (default: false)
# WHISPER_ENABLED=false

# Whisper API key (defaults to OPENAI_API_KEY)
# WHISPER_API_KEY=your_openai_api_key_here

# Whisper API base URL (defaults to OPENAI_BASE_URL)
# WHISPER_BASE_URL=https://api.openai.com/v1

# Whisper model (default: whisper-1)
# WHISPER_MODEL=whisper-1

# Use local whisper model instead of API (requires openai-whisper package)
# USE_LOCAL_WHISPER=false
# LOCAL_WHISPER_MODEL=base

# Language hint for transcription (auto-detect if not set)
# WHISPER_LANGUAGE=en

# =============================================================================
# Optional: Orchestrator Configuration
# =============================================================================

# Orchestrator URL (default: http://localhost:8085)
# ORCHESTRATOR_URL=http://localhost:8085

# Agent identification (auto-detected if not set)
# AGENT_POD_IP=10.0.0.5
# AGENT_POD_PORT=8001
# AGENT_HOSTNAME=agent-pod-1

# Agent config to load (default: creator)
# AGENT_CONFIG=creator

# =============================================================================
# Optional: Workspace
# =============================================================================

# Base path for job workspaces (default: ./workspace)
# WORKSPACE_PATH=./workspace

# =============================================================================
# Optional: Citation Engine
# =============================================================================

# Citation database (defaults to DATABASE_URL)
# CITATION_DB_URL=postgresql://graphrag:graphrag_password@localhost:5432/graphrag

# Citation LLM endpoint (defaults to LLM_BASE_URL)
# CITATION_LLM_URL=http://localhost:8080/v1
# CITATION_LLM_MODEL=gpt-4
# Reasoning level also controls analysis depth
# CITATION_REASONING_REQUIRED=low

# =============================================================================
# Optional: Service Ports (for docker-compose)
# =============================================================================

# Override default ports if needed
# POSTGRES_PORT=5432
# NEO4J_BOLT_PORT=7687
# NEO4J_HTTP_PORT=7474
# MONGODB_PORT=27017
# ORCHESTRATOR_PORT=8085
# COCKPIT_PORT=4000
# AGENT_PORT=8001
# MCP_PORT=8000

# =============================================================================
# Optional: Logging & Debugging
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR (default: INFO)
# LOG_LEVEL=INFO

# Stream LLM tokens to stderr (useful for debugging)
# DEBUG_LLM_STREAM=1
# DEBUG_LLM_TAIL=500

# Show verbose token breakdown per message (very noisy, for context debugging)
# DEBUG_TOKEN_BREAKDOWN=false
