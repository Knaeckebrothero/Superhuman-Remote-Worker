# =============================================================================
# Graph-RAG Autonomous Agent System - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values

# =============================================================================
# Database Configuration
# =============================================================================

# Neo4j Database (Knowledge Graph)
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=neo4j_password

# PostgreSQL Database (Shared State / Job Tracking)
# Required for the two-agent system
DATABASE_URL=postgresql://graphrag:graphrag_password@localhost:5432/graphrag

# Individual PostgreSQL connection parameters (used by docker-compose.dbs.yml)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=graphrag
POSTGRES_PASSWORD=graphrag_password
POSTGRES_DB=graphrag

# Optional: MongoDB for LLM request archiving
# MONGODB_URL=mongodb://localhost:27017/graphrag_logs

# =============================================================================
# LLM Configuration
# =============================================================================

# OpenAI API Key (or compatible API)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Custom OpenAI-compatible API endpoint (e.g., vLLM, Ollama, llama.cpp)
# Leave unset to use OpenAI's API directly
# LLM_BASE_URL=http://your-server:8000/v1

# =============================================================================
# Citation Engine Configuration
# =============================================================================

# Citation Engine database (can share with main DATABASE_URL)
CITATION_DB_URL=postgresql://graphrag:graphrag_password@localhost:5432/graphrag

# Citation Engine LLM endpoint
CITATION_LLM_URL=http://localhost:8080/v1
CITATION_LLM_MODEL=gpt-oss-120b

# Reasoning level: low, medium, high
CITATION_REASONING_REQUIRED=low

# =============================================================================
# External Services
# =============================================================================

# Web Search (Tavily) - Required for Creator Agent research
# Get your API key at https://tavily.com/
TAVILY_API_KEY=your_tavily_api_key_here

# =============================================================================
# Agent Configuration
# =============================================================================

# Workspace base path for agent file storage
# Default: ./workspace in development, /workspace in containers
# Example: .workspace (creates .workspace/job_{uuid}/)
# WORKSPACE_PATH=.workspace

# Iteration Limits (for long document processing)
# These control how many LLM iterations an agent can run before stopping.
# Default of 500 supports ~50 page documents taking 1-2 hours.
CREATOR_MAX_ITERATIONS=500
VALIDATOR_MAX_ITERATIONS=500

# LangGraph Recursion Limits
# These limit the graph traversal depth. Should be >= max iterations.
CREATOR_RECURSION_LIMIT=500
VALIDATOR_RECURSION_LIMIT=200

# Polling intervals (in seconds)
CREATOR_POLLING_INTERVAL=30
VALIDATOR_POLLING_INTERVAL=10

# Retry configuration
MAX_REQUIREMENT_RETRIES=5
AGENT_RETRY_DELAY=10

# Thresholds
MIN_CONFIDENCE_THRESHOLD=0.6
DUPLICATE_SIMILARITY_THRESHOLD=0.95
FULFILLMENT_CONFIDENCE_THRESHOLD=0.7

# Context management (token limits)
CONTEXT_COMPACTION_THRESHOLD=100000
CONTEXT_MAX_OUTPUT_TOKENS=80000

# Agent API URLs (for dashboard HTTP client)
# These are used by the Streamlit dashboard to connect to deployed agents
CREATOR_AGENT_URL=http://localhost:8001
VALIDATOR_AGENT_URL=http://localhost:8002

# HTTP client timeout (in seconds)
AGENT_TIMEOUT=30.0

# =============================================================================
# Observability
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# =============================================================================
# Notes
# =============================================================================
# LLM model settings (model name, temperature, max_iterations, reasoning_level)
# are configured in config/llm_config.json - not in environment variables.
#
# Agent-specific settings (polling intervals, thresholds) are also in
# config/llm_config.json under creator_agent, validator_agent, and orchestrator.
