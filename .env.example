# =============================================================================
# Graph-RAG Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values

# =============================================================================
# Required: LLM Configuration
# =============================================================================

# OpenAI API Key (or compatible API)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Custom OpenAI-compatible endpoint (vLLM, Ollama, llama.cpp)
# Leave unset to use OpenAI's API directly
# LLM_BASE_URL=http://your-server:8000/v1

# =============================================================================
# Required: PostgreSQL Database
# =============================================================================

# Connection URL (preferred)
DATABASE_URL=postgresql://graphrag:graphrag_password@localhost:5432/graphrag

# Or individual parameters (used if DATABASE_URL not set)
# POSTGRES_HOST=localhost
# POSTGRES_USER=graphrag
# POSTGRES_PASSWORD=graphrag_password
# POSTGRES_DB=graphrag

# =============================================================================
# Optional: Additional Services
# =============================================================================

# Neo4j (Knowledge Graph) - only if connections.neo4j: true in config
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=neo4j_password

# MongoDB (LLM request archiving) - enables audit trail
# MONGODB_URL=mongodb://localhost:27017/graphrag_logs

# Tavily (Web Search) - required for web_search tool
# TAVILY_API_KEY=your_tavily_api_key_here

# Unpaywall (Open Access paper lookup) - required for download_paper fallback
# Register at https://unpaywall.org/products/api (free, just needs email)
# UNPAYWALL_EMAIL=your@email.com

# Semantic Scholar (paper metadata & citations) - optional, for higher rate limits
# Register at https://www.semanticscholar.org/product/api#api-key
# SEMANTIC_SCHOLAR_API_KEY=your_semantic_scholar_api_key_here

# Anthropic (Claude models) - required when using claude-* models
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google (Gemini models) - required when using gemini-* models
# GOOGLE_API_KEY=your_google_api_key_here

# Groq (Fast inference) - required when provider: groq
# GROQ_API_KEY=your_groq_api_key_here

# =============================================================================
# Optional: Vision Model (for describing visual content)
# =============================================================================
# Used when the primary model is text-only (llm.multimodal: false in config).
# A separate multimodal model generates text descriptions of images and
# document pages (charts, diagrams, figures).

# By default, vision requests go to OpenAI (https://api.openai.com/v1) using
# OPENAI_API_KEY. This is independent of LLM_BASE_URL, so you can use a custom
# endpoint for the main LLM while vision still uses OpenAI's gpt-4o-mini.

# Vision model API key (defaults to OPENAI_API_KEY)
# VISION_API_KEY=your_openai_api_key_here

# Vision model base URL (defaults to OpenAI: https://api.openai.com/v1)
# Only set this if you have a vision-capable model on a different endpoint
# VISION_BASE_URL=https://api.openai.com/v1

# Vision model to use (default: gpt-4o-mini - fast and cost-effective)
# Options: gpt-4o-mini, gpt-4o, gpt-4-turbo, or any multimodal model
# VISION_MODEL=gpt-4o-mini

# Request timeout in seconds (default: 120)
# VISION_TIMEOUT=120

# =============================================================================
# Optional: Audio Transcription (Whisper)
# =============================================================================
# Used for transcribing audio files. Supports OpenAI's Whisper API or
# local whisper model as fallback.
#
# Note: Audio transcription is not yet implemented in read_file tool.
# These settings are reserved for future use.

# Enable audio transcription (default: false)
# WHISPER_ENABLED=false

# Whisper API key (defaults to OPENAI_API_KEY)
# WHISPER_API_KEY=your_openai_api_key_here

# Whisper API base URL (defaults to OPENAI_BASE_URL)
# WHISPER_BASE_URL=https://api.openai.com/v1

# Whisper model (default: whisper-1)
# WHISPER_MODEL=whisper-1

# Use local whisper model instead of API (requires openai-whisper package)
# USE_LOCAL_WHISPER=false
# LOCAL_WHISPER_MODEL=base

# Language hint for transcription (auto-detect if not set)
# WHISPER_LANGUAGE=en

# =============================================================================
# Optional: Browser Automation (browser-use + Playwright)
# =============================================================================
# Used for navigating websites, downloading files from publisher pages,
# and interacting with dynamic web content.
# Requires: pip install browser-use && playwright install chromium

# LLM model for browser agent (default: gpt-4o-mini)
# BROWSER_LLM_MODEL=gpt-4o-mini

# API key for browser LLM (defaults to OPENAI_API_KEY)
# BROWSER_LLM_API_KEY=your_openai_api_key_here

# Base URL for browser LLM (defaults to LLM_BASE_URL)
# BROWSER_LLM_BASE_URL=https://api.openai.com/v1

# Run browser without visible UI (default: true)
# BROWSER_HEADLESS=true

# =============================================================================
# Optional: Research Proxy (for institutional access)
# =============================================================================
# Route browser and download requests through a proxy to access paywalled
# content via university VPN or SSH tunnel.
#
# Example with SSH tunnel:
#   ssh -D 1080 -N user@university.edu
#   RESEARCH_PROXY_TYPE=socks5
#   RESEARCH_PROXY_HOST=localhost
#   RESEARCH_PROXY_PORT=1080

# Proxy type: "http", "socks5", or "none" (default: none)
# RESEARCH_PROXY_TYPE=none

# Proxy host and port
# RESEARCH_PROXY_HOST=localhost
# RESEARCH_PROXY_PORT=1080

# Optional proxy authentication
# RESEARCH_PROXY_USER=
# RESEARCH_PROXY_PASS=

# =============================================================================
# Optional: Gitea (Workspace Delivery)
# =============================================================================
# Internal git server for workspace access. Agents push workspace contents
# after each phase, allowing users to browse deliverables via Gitea web UI.

# GITEA_ADMIN_USER=graphrag
# GITEA_ADMIN_PASSWORD=graphrag_gitea
# GITEA_URL=http://gitea:3000
# GITEA_PORT=3000

# =============================================================================
# Optional: Orchestrator Configuration
# =============================================================================

# Orchestrator URL (default: http://localhost:8085)
# ORCHESTRATOR_URL=http://localhost:8085

# Agent identification (auto-detected if not set)
# AGENT_POD_IP=10.0.0.5
# AGENT_POD_PORT=8001
# AGENT_HOSTNAME=agent-pod-1

# Agent config to load (default: creator)
# AGENT_CONFIG=creator

# =============================================================================
# Optional: Workspace
# =============================================================================

# Base path for job workspaces (default: ./workspace)
# WORKSPACE_PATH=./workspace

# =============================================================================
# Optional: Citation Engine
# =============================================================================

# Citation database (defaults to DATABASE_URL)
# CITATION_DB_URL=postgresql://graphrag:graphrag_password@localhost:5432/graphrag

# Citation LLM endpoint (defaults to LLM_BASE_URL)
# CITATION_LLM_URL=http://localhost:8080/v1
# CITATION_LLM_MODEL=gpt-4
# Reasoning level also controls analysis depth
# CITATION_REASONING_REQUIRED=low

# =============================================================================
# Optional: Service Ports (for docker-compose)
# =============================================================================

# Override default ports if needed
# POSTGRES_PORT=5432
# NEO4J_BOLT_PORT=7687
# NEO4J_HTTP_PORT=7474
# MONGODB_PORT=27017
# ORCHESTRATOR_PORT=8085
# COCKPIT_PORT=4000
# AGENT_PORT=8001
# MCP_PORT=8000

# =============================================================================
# Optional: Logging & Debugging
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR (default: INFO)
# LOG_LEVEL=INFO

# Stream LLM tokens to stderr (useful for debugging)
# DEBUG_LLM_STREAM=1
# DEBUG_LLM_TAIL=500

# Show verbose token breakdown per message (very noisy, for context debugging)
# DEBUG_TOKEN_BREAKDOWN=false
